{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALMerrill/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Kp-azNgihL",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALMerrill/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Lab 4: Cancer Detection\n",
        "\n",
        "## Objective\n",
        "* To build a dense prediction model\n",
        "* To begin reading current papers in DNN research\n",
        "\n",
        "## Deliverable\n",
        "For this lab, you will turn in a notebook that describes your efforts at creating\n",
        "a pytorch radiologist. Your final deliverable is a notebook that has (1) deep network,\n",
        "(2) cost function, (3) method of calculating accuracy,\n",
        "(4) an image that shows the dense prediction produced by your network on the pos_test_000072.png image.\n",
        "This is an image in the test set that your network will not have seen before.\n",
        "This image, and the ground truth labeling, is shown below.\n",
        "(And is contained in the downloadable dataset below).\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n",
        "<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n",
        "\n",
        "\n",
        "## Grading standards\n",
        "Your notebook will be graded on the following:\n",
        "* 40% Proper design, creation and debugging of a dense prediction network\n",
        "* 40% Proper implementation of a loss function and train/test set accuracy measure\n",
        "* 10% Tidy visualizations of loss of your dense predictor during training\n",
        "* 10% Test image output\n",
        "\n",
        "\n",
        "## Data set\n",
        "The data is given as a set of 1024×1024 PNG images. Each input image (in \n",
        "the ```inputs``` directory) is an RGB image of a section of tissue,\n",
        "and there a file with the same name (in the ```outputs``` directory) \n",
        "that has a dense labeling of whether or not a section of tissue is cancerous\n",
        "(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n",
        "\n",
        "The data has been pre-split for you into test and training splits.\n",
        "Filenames also reflect whether or not the image has any cancer at all \n",
        "(files starting with ```pos_``` have some cancerous pixels, while files \n",
        "starting with ```neg_``` have no cancer anywhere).\n",
        "All of the data is hand-labeled, so the dataset is not very large.\n",
        "That means that overfitting is a real possibility.\n",
        "\n",
        "## Description\n",
        "For a video including some tips and tricks that can help with this lab: [https://youtu.be/Ms19kgK_D8w](https://youtu.be/Ms19kgK_D8w)\n",
        "For this lab, you will implement a virtual radiologist.\n",
        "You are given images of possibly cancerous tissue samples, \n",
        "and you must build a detector that identifies where in the tissue cancer may reside.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 0\n",
        "Watch and follow video tutorial:\n",
        "\n",
        "https://youtu.be/Ms19kgK_D8w\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Watch tutorial\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "### Part 1\n",
        "Implement a dense predictor\n",
        "\n",
        "In previous labs and lectures, we have talked about DNNs that classify an \n",
        "entire image as a single class. Here, however, we are interested in a more nuanced classification: \n",
        "given an input image, we would like to identify each pixel that is possibly cancerous. \n",
        "That means that instead of a single output, your network should output an “image”, \n",
        "where each output pixel of your network represents the probability that a pixel is cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Create a Network that classifies each pixel as a 1 or 0 for cancerous / not cancerous\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "___\n",
        "\n",
        "### Part 1a\n",
        "Implement your network topology\n",
        "\n",
        "\n",
        "Use the “Deep Convolution U-Net” from this paper: [(U-Net: Convolutional Networks for Biomedical Image Segmentation)](https://arxiv.org/pdf/1505.04597.pdf) \n",
        "\n",
        "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
        "\n",
        "You should use existing pytorch functions (not your own Conv2D module), such as ```nn.Conv2d```;\n",
        "you will also need the pytorch function ```torch.cat``` and ```nn.ConvTranspose2d```\n",
        "\n",
        "```torch.cat``` allows you to concatenate tensors.\n",
        "```nn.ConvTranspose2d``` is the opposite of ```nn.Conv2d```.\n",
        "It is used to bring an image from low res to higher res.\n",
        "[This blog](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) should help you understand this function in detail.\n",
        "\n",
        "Note that the simplest network you could implement (with all the desired properties)\n",
        "is just a single convolution layer with two filters and no relu! \n",
        "Why is that? (of course it wouldn't work very well!)\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Understand the U-Net architecture\n",
        "* Understand concatenation of inputs from multiple prior layers\n",
        "* Understand ConvTranspose\n",
        "* Answer Question / Reflect on simplest network with the desired properties\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "___\n",
        "The intention of this lab is to learn how to make deep neural nets and implement loss function.\n",
        "Therefore we'll help you with the implementation of Dataset.\n",
        "This code will download the dataset for you so that you are ready to use it and focus on network\n",
        "implementation, losses and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQOefmcZVgTl",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# !pip3 install torch\n",
        "# !pip3 install torchvision\n",
        "# !pip3 install tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Il_53HLSWPTY",
        "colab": {}
      },
      "source": [
        "#log max trick (also picture on phone)\n",
        "#     maxes, _ = torch.max(y_hat, 1, keepdim=True)\n",
        "#     y_hat = y_hat - maxes\n",
        "#     y_hat = torch.exp(y_hat)\n",
        "#     y_hat = y_hat / torch.sum(y_hat, 1, keepdim=True)\n",
        "\n",
        "\n",
        "class CancerDataset(Dataset):\n",
        "  def __init__(self, root, download=True, size=512, train=True):\n",
        "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
        "#       datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
        "      datasets.utils.download_url('https://nolans-cs-bucket.s3-us-west-1.amazonaws.com/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
        "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
        "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
        "    \n",
        "    postfix = 'train' if train else 'test'\n",
        "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
        "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_gzip(gzip_path, remove_finished=False):\n",
        "    print('Extracting {}'.format(gzip_path))\n",
        "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
        "      out_f.write(zip_f.read())\n",
        "    if remove_finished:\n",
        "      os.unlink(gzip_path)\n",
        "  \n",
        "  @staticmethod\n",
        "  def extract_tar(tar_path):\n",
        "    print('Untarring {}'.format(tar_path))\n",
        "    z = tarfile.TarFile(tar_path)\n",
        "    z.extractall(tar_path.replace('.tar', ''))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = self.dataset_folder[index]\n",
        "    label = self.label_folder[index]\n",
        "    return img[0],label[0][0]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return 100\n",
        "#     return len(self.dataset_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 1b\n",
        "Implement a cost function\n",
        "\n",
        "You should still use cross-entropy as your cost function, but you may need to think hard about how exactly to set this up – your network should output cancer/not-cancer probabilities for each pixel, which can be viewed as a two-class classification problem.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Adapt CrossEntropyLoss for 2 class pixel classification\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPgrP88aOtfy",
        "colab": {}
      },
      "source": [
        "# You'll probably want a function or something to test input / output sizes of the ConvTranspose2d layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jq22IyKanxo_",
        "colab": {}
      },
      "source": [
        "# Since you will be using the output of one network in two places(convolution and maxpooling),\n",
        "# you can't use nn.Sequential.\n",
        "# Instead you will write up the network like normal variable assignment as the example shown below:\n",
        "# You are welcome (and encouraged) to use the built-in batch normalization and dropout layer.\n",
        "\n",
        "# TODO: You need to change this to fit the UNet structure!!!\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "#     self.poolConv = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 2, padding = 1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out1 = F.relu(self.conv1(x))\n",
        "    out2 = F.relu(self.conv2(out1))\n",
        "    return out2\n",
        "#     return F.relu(self.poolConv(out2))  #probably don't want relu on final layer\n",
        "  \n",
        "  \n",
        "class ConvTransposeBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvTransposeBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.convT1 = nn.ConvTranspose2d(out_channels, out_channels // 2, kernel_size = 4, stride = 2, padding = 1) #Is it fine to just divide the channels by 2? That way it comes out to the same when the other gets concatted on\n",
        "    \n",
        "  def forward(self, x, skip_connection=torch.Tensor([]).cuda(), last_layer=False):\n",
        "    x = torch.cat((skip_connection, x), 1)\n",
        "    out1 = F.relu(self.conv1(x))\n",
        "    out2 = F.relu(self.conv2(out1))\n",
        "    if last_layer:\n",
        "      return out2\n",
        "    out3 = F.relu(self.convT1(out2))\n",
        "    return out3\n",
        "      \n",
        "\n",
        "  \n",
        "class CancerDetection(nn.Module): #lab video at 39:00 for a bit of a description of the layout for these\n",
        "  def __init__(self):\n",
        "    super(CancerDetection, self).__init__()\n",
        "    self.block1 = ConvBlock(3, 64)\n",
        "    self.pool1 = nn.Conv2d(64, 64, kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.block2 = ConvBlock(64, 128)\n",
        "    self.pool2 = nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.block3 = ConvBlock(128, 256)\n",
        "    self.pool3 = nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.block4 = ConvBlock(256, 512)\n",
        "    self.pool4 = nn.Conv2d(512, 512, kernel_size = 3, stride = 2, padding = 1)\n",
        "    #UpConv\n",
        "    self.block5 = ConvTransposeBlock(512, 1024)\n",
        "    self.block6 = ConvTransposeBlock(1024, 512)\n",
        "    self.block7 = ConvTransposeBlock(512, 256)\n",
        "    self.block8 = ConvTransposeBlock(256, 128)\n",
        "    self.block9 = ConvTransposeBlock(128, 64)\n",
        "    self.conv1x1 = nn.Conv2d(64, 2, kernel_size=1, stride=1)\n",
        "\n",
        "    \n",
        " \n",
        "  def forward(self, x):\n",
        "    print('\\nshapes')\n",
        "    print('x:',x.size())\n",
        "    out1 = self.block1(x)\n",
        "    pool1 = F.relu(self.pool1(out1))\n",
        "    out2 = self.block2(pool1)\n",
        "    pool2 = F.relu(self.pool2(out2))\n",
        "    out3 = self.block3(pool2)\n",
        "    pool3 = F.relu(self.pool3(out3))\n",
        "    out4 = self.block4(pool3)\n",
        "    pool4 = F.relu(self.pool4(out4))\n",
        "    #UpConv\n",
        "    out5 = self.block5(pool4)\n",
        "    out6 = self.block6(out5, skip_connection=out4)\n",
        "    out7 = self.block7(out6, skip_connection=out3)\n",
        "    out8 = self.block8(out7, skip_connection=out2)\n",
        "    out9 = self.block9(out8, skip_connection=out1, last_layer=True)\n",
        "    out10 = self.conv1x1(out9) #Make sure it is back to image size before this\n",
        "    print('final:', out10.size())\n",
        "    return out10\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NAjagHCdGNAh",
        "colab": {}
      },
      "source": [
        "# Create your datasets and neural network as you have before\n",
        "\n",
        "train_dataset = CancerDataset('root')\n",
        "val_dataset = CancerDataset('root', train=False)\n",
        "model = CancerDetection()\n",
        "model.cuda()\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         num_workers =4, #check iterations/sec in tqdm to see how this helps or hurts\n",
        "                         pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                       batch_size=batch_size,\n",
        "                       num_workers =4,\n",
        "                       pin_memory=True)\n",
        "\n",
        "data_loaders = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "losses = []\n",
        "validations = []\n",
        "train_acc = []\n",
        "val_acc = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkieTbwlYWPS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3754243a-305d-483e-ec4e-eac0cf753b4f"
      },
      "source": [
        "# This is what was talked about in the video for memory management\n",
        "\n",
        "def hasCancerCells(y_truth):\n",
        "  return len(y_truth.nonzero()) > 0\n",
        "\n",
        "def accuracy(y_hat, y_truth):\n",
        "  # y_hat: batch x 2 x 512 x 512   Each channel holds the probabilities for that class\n",
        "  # y_truth: batch x 1 x 512 x 512\n",
        "  y_hat = y_hat[0]\n",
        "  y_truth = y_truth[0]\n",
        "  y_hat[0] = y_hat[0] > .5\n",
        "  y_hat[1] = y_hat[1] >= .5\n",
        "  if hasCancerCells(y_truth):\n",
        "    intersection = torch.sum(y_hat[1] * y_truth)\n",
        "    union = torch.sum(y_hat[1] + y_truth) - intersection\n",
        "  else:\n",
        "    intersection = torch.sum(y_hat[0] * y_truth)\n",
        "    union = torch.sum(y_hat[0] + y_truth) - intersection\n",
        "\n",
        "  if union == 0:\n",
        "    raise Exception(\"Divide by zero in IoU\")\n",
        "  return intersection / union\n",
        "  \n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "def train():\n",
        "  for epoch in range(EPOCHS):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "  \n",
        "    for batch, (x, y_truth) in enumerate(train_loader):\n",
        "      x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x) \n",
        "      loss = objective(y_hat, y_truth.long())\n",
        "      loss.backward()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      acc = accuracy(y_hat, y_truth)\n",
        "      train_acc.append(acc)\n",
        "      mem = torch.cuda.memory_allocated(0) / 1e9\n",
        "\n",
        "      loop.set_description('epoch: {}, loss: {:.4f}, accuracy: {:.3f}, mem: {:.2f}'\n",
        "                           .format(epoch, loss, acc, mem))\n",
        "      loop.update(1)\n",
        "      optimizer.step()\n",
        "      if batch % 20 == 0:\n",
        "        y_hat_val = np.mean([accuracy(model(x.cuda()), y_truth.cuda()) for x, y_truth in val_loader])\n",
        "        val_acc.append((len(losses), y_hat_val))\n",
        "        val = np.mean([objective(model(x.cuda()), y.cuda()).item() \n",
        "                       for x, y in val_loader])\n",
        "        validations.append((len(losses), val))\n",
        "    loop.close()\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    #your code for calling dataset and dataloader\n",
        "    gc.collect()\n",
        "    print(torch.cuda.memory_allocated(0) / 1e9)\n",
        "    \n",
        "    train()\n",
        "    # Call your model, figure out loss and accuracy\n",
        "    \n",
        "  except:\n",
        "    __ITB__()\n",
        "    \n",
        "scope()\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.702268416\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, loss: 0.0000, accuracy: 0.000, mem: 0.71:   1%|          | 1/100 [00:01<02:30,  1.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "shapes\n",
            "x: torch.Size([1, 3, 512, 512])\n",
            "final: torch.Size([1, 2, 512, 512])\n",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-137-1965d3b5b0d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m     46\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     47\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_hat_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36my_hat_val\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mnp.mean\u001b[0m \u001b[0;34m= <function mean at 0x7f630b9ffbf8>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36maccuracy\u001b[0m \u001b[0;34m= <function accuracy at 0x7f62a6026730>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= CancerDetection(\n",
            "  (block1): ConvBlock(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block2): ConvBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block3): ConvBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block4): ConvBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block5): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block6): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block7): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block8): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block9): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (conv1x1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mx.cuda\u001b[0m \u001b[0;34m= <built-in method cuda of Tensor object at 0x7f62a60c5168>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_truth.cuda\u001b[0m \u001b[0;34m= <built-in method cuda of Tensor object at 0x7f62a60c5090>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mx\u001b[0m \u001b[0;34m= tensor([[[[0.9490, 0.9569, 0.9490,  ..., 0.4941, 0.8275, 0.9412],\n",
            "          [0.9490, 0.9529, 0.9451,  ..., 0.4510, 0.7333, 0.8980],\n",
            "          [0.9451, 0.9490, 0.9451,  ..., 0.4902, 0.6314, 0.8588],\n",
            "          ...,\n",
            "          [0.7843, 0.8980, 0.8863,  ..., 0.4980, 0.4745, 0.3804],\n",
            "          [0.8667, 0.8980, 0.8157,  ..., 0.7451, 0.5373, 0.3686],\n",
            "          [0.9686, 0.9255, 0.8392,  ..., 0.9098, 0.6824, 0.4118]],\n",
            "\n",
            "         [[0.9412, 0.9490, 0.9529,  ..., 0.4118, 0.6235, 0.7490],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.3804, 0.5765, 0.6000],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.4039, 0.4824, 0.5412],\n",
            "          ...,\n",
            "          [0.7961, 0.8863, 0.8863,  ..., 0.4980, 0.4667, 0.4235],\n",
            "          [0.8627, 0.8902, 0.8196,  ..., 0.7059, 0.5255, 0.3843],\n",
            "          [0.9490, 0.9176, 0.8353,  ..., 0.8745, 0.6510, 0.4196]],\n",
            "\n",
            "         [[0.9412, 0.9412, 0.9490,  ..., 0.5216, 0.6627, 0.7686],\n",
            "          [0.9373, 0.9529, 0.9451,  ..., 0.5059, 0.6157, 0.6588],\n",
            "          [0.9412, 0.9529, 0.9373,  ..., 0.5412, 0.5490, 0.5804],\n",
            "          ...,\n",
            "          [0.7647, 0.8510, 0.8627,  ..., 0.6118, 0.5922, 0.5490],\n",
            "          [0.8471, 0.8667, 0.7804,  ..., 0.7647, 0.6314, 0.5216],\n",
            "          [0.9412, 0.9020, 0.8196,  ..., 0.8745, 0.7333, 0.5255]]]],\n",
            "       device='cuda:0')\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_truth\u001b[0m \u001b[0;34m= tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mval_loader\u001b[0m \u001b[0;34m= <torch.utils.data.dataloader.DataLoader object at 0x7f62a6114668>\u001b[0m\n",
            "\u001b[1;32m     49\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     50\u001b[0m         val = np.mean([objective(model(x.cuda()), y.cuda()).item() \n",
            "\n",
            "\u001b[0;32m<ipython-input-137-1965d3b5b0d6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0=<torch.utils.data.dataloader._DataLoaderIter object>)\u001b[0m\n",
            "\u001b[1;32m     46\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     47\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_hat_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36my_hat_val\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mnp.mean\u001b[0m \u001b[0;34m= <function mean at 0x7f630b9ffbf8>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36maccuracy\u001b[0m \u001b[0;34m= <function accuracy at 0x7f62a6026730>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= CancerDetection(\n",
            "  (block1): ConvBlock(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block2): ConvBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block3): ConvBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block4): ConvBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (block5): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block6): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block7): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block8): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (block9): ConvTransposeBlock(\n",
            "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (convT1): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            "  (conv1x1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mx.cuda\u001b[0m \u001b[0;34m= <built-in method cuda of Tensor object at 0x7f62a6123360>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_truth.cuda\u001b[0m \u001b[0;34m= <built-in method cuda of Tensor object at 0x7f62a5f02e58>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mx\u001b[0m \u001b[0;34m= tensor([[[[0.9529, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9569, 0.9569,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9490, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.9451, 0.9490, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9529, 0.9569,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9569, 0.9529, 0.9490,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.9569, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9569, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9569, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.9529, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9529, 0.9529,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.9490, 0.9529, 0.9412,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9529, 0.9490,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9490, 0.9529,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.9608, 0.9529, 0.9569,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9529, 0.9451, 0.9569,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.9451, 0.9412, 0.9569,  ..., 0.0000, 0.0000, 0.0000]]]])\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_truth\u001b[0m \u001b[0;34m= tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mval_loader\u001b[0m \u001b[0;34m= <torch.utils.data.dataloader.DataLoader object at 0x7f62a6114668>\u001b[0m\n",
            "\u001b[1;32m     49\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     50\u001b[0m         val = np.mean([objective(model(x.cuda()), y.cuda()).item() \n",
            "\n",
            "\u001b[0;32m<ipython-input-137-1965d3b5b0d6>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(y_hat=tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "       ...ice='cuda:0',\n",
            "       grad_fn=<AsStridedBackward>), y_truth=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'))\u001b[0m\n",
            "\u001b[1;32m     14\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     15\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mintersection\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch.sum\u001b[0m \u001b[0;34m= <built-in method sum of type object at 0x7f62f4808b80>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_hat\u001b[0m \u001b[0;34m= tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
            "       grad_fn=<AsStridedBackward>)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36my_truth\u001b[0m \u001b[0;34m= tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\u001b[0m\n",
            "\u001b[1;32m     17\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 8.87 GiB already allocated; 1.81 MiB free; 1.96 GiB cached)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoJ1TutGwfMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CZ062Jv1jIIu"
      },
      "source": [
        "\n",
        "___\n",
        "\n",
        "### Part 2\n",
        "\n",
        "Plot performance over time\n",
        "\n",
        "Please generate a plot that shows loss on the training set as a function of training time. Make sure your axes are labeled!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Plot training loss as function of training time (not Epochs)\n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTg1jyIsYVZN",
        "colab": {}
      },
      "source": [
        "# Your plotting code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S4s92S2_jQOG"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 3\n",
        "\n",
        "Generate a prediction on the pos_test_000072.png image\n",
        "\n",
        "Calculate the output of your trained network on the pos_test_000072.png image,\n",
        "then make a hard decision (cancerous/not-cancerous) for each pixel.\n",
        "The resulting image should be black-and-white, where white pixels represent things\n",
        "you think are probably cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "Guessing that the pixel is not cancerous every single time will give you an accuracy of ~ 85%.\n",
        "Your trained network should be able to do better than that (but you will not be graded on accuracy).\n",
        "This is the result I got after 1 hour or training.\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXfG3wClh8an",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Code for testing prediction on an image\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}