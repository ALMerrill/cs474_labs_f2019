{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE8ElXCulbYL",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Lab 5: Style Transfer\n",
        "\n",
        "## Objective\n",
        "To explore an alternative use of DNNs by implementing the style transfer algorithm.\n",
        "To understand the importance of a complex loss function.\n",
        "To see how we can optimize not only over network parameters,\n",
        "but over other objects (such as images) as well.\n",
        "\n",
        "## Deliverable\n",
        "For this lab, you will need to implement the style transfer algorithm of Gatys et al.\n",
        "\n",
        "* You must extract statistics from the content and style images\n",
        "* You must formulate an optimization problem over an input image\n",
        "* You must optimize the image to match both style and content\n",
        "\n",
        "In your jupyter notebook, you should turn in the following:\n",
        "\n",
        "* The final image that you generated\n",
        "* Your code\n",
        "* A description of the equations from (Gatys et. al. 2016) -- see the bottom of the notebook for details.\n",
        "\n",
        "An example image that I generated is shown below\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=300&tok=179805&media=cs501r_f2016:style1.png)\n",
        "\n",
        "## Grading standards\n",
        "Your code will be graded on the following:\n",
        "\n",
        "* 35% Correct extraction of statistics\n",
        "* 35% Correct construction of loss function in a loss class\n",
        "* 10% Plain English description of equations from (Gatys et. al. 2016) at the bottom of the notebook\n",
        "* 10% Correct initialization and optimization of image variable in a dataset class\n",
        "* 10% Awesome looking final image\n",
        "\n",
        "Note: You may reference other implementations for ideas, but you are on your honor not to copy/paste other people's code.\n",
        "\n",
        "## Description:\n",
        "\n",
        "For this lab, you should implement the style transfer algorithm referenced above.\n",
        "To do this, you will need to unpack the given images.\n",
        "Since we want you to focus on implementing the paper and the loss function, \n",
        "we will give you the code for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VdQhJmJJYOt_",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from PIL import Image\n",
        "import io\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose', color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fvGqSVnGec12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "190e73f1-3b98-40af-d3d9-8e54d97f5079"
      },
      "source": [
        "load_and_normalize = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def upload():\n",
        "  print('Upload Content Image')\n",
        "  file_dict = files.upload()\n",
        "  content_path = io.BytesIO(file_dict[next(iter(file_dict))])\n",
        "\n",
        "  print('\\nUpload Style Image')\n",
        "  file_dict = files.upload()\n",
        "  style_path = io.BytesIO(file_dict[next(iter(file_dict))])\n",
        "  return content_path, style_path\n",
        "\n",
        "content_path, style_path = upload()\n",
        "\n",
        "print(\"Content Path: {}\".format(content_path))\n",
        "print(\"Style Path: {}\".format(style_path))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Upload Content Image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c30ae3ad-9330-46bf-8bea-c316578fe699\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c30ae3ad-9330-46bf-8bea-c316578fe699\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-14185f02fea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content Path: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-14185f02fea3>\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Upload Content Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mcontent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: google.colab._files is undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IxKpna4llbYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After the images are uploaded on to the local filesystem, you can use:\n",
        "content_image_orig = Image.open(content_path)\n",
        "content_image = load_and_normalize(np.array(content_image_orig)).unsqueeze(0).cuda()\n",
        "style_image_orig = Image.open(style_path)\n",
        "style_image = load_and_normalize(np.array(style_image_orig)).unsqueeze(0).cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rf0fV_SSSKEo"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 1\n",
        "Create a class to extract the layers needed for statistics\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Use the pretrained VGG in your model\n",
        "* Gather statistics from the outputs of intermediate layers for the content image\n",
        "* Gather statistics for the style image\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "* Use the pretrained VGG in your model\n",
        "* Gather statistics from the outputs of intermediate layers for the content image\n",
        "* Gather statistics for the style image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nonAvCqYgL6",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean=torch.tensor([0.485, 0.456, 0.406]).cuda(), std=torch.tensor([0.229, 0.224, 0.225]).cuda()):\n",
        "      super(Normalization, self).__init__()\n",
        "      self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "      self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "      return (img - self.mean) / self.std\n",
        "\n",
        "class VGGIntermediate(nn.Module):\n",
        "  def __init__(self, requested=[]):\n",
        "    super(VGGIntermediate, self).__init__()\n",
        "    self.norm = Normalization().eval()\n",
        "    self.intermediates = {}\n",
        "    self.vgg = models.vgg19(pretrained=True).features.eval()\n",
        "    for i, m in enumerate(self.vgg.children()):\n",
        "        if isinstance(m, nn.ReLU):   # we want to set the relu layers to NOT do the relu in place. \n",
        "          m.inplace = False          # the model has a hard time going backwards on the in place functions. \n",
        "        \n",
        "        if i in requested:\n",
        "          def curry(i):\n",
        "            def hook(module, input, output):\n",
        "              self.intermediates[i] = output\n",
        "            return hook\n",
        "          m.register_forward_hook(curry(i))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.vgg(self.norm(x))  \n",
        "    return self.intermediates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "deG45N1AYi6O",
        "colab": {}
      },
      "source": [
        "vgg_names = [\"conv1_1\", \"relu1_1\", \"conv1_2\", \"relu1_2\", \"maxpool1\", \"conv2_1\", \"relu2_1\", \"conv2_2\", \"relu2_2\", \"maxpool2\", \"conv3_1\", \"relu3_1\", \"conv3_2\", \"relu3_2\", \"conv3_3\", \"relu3_3\",\"maxpool3\", \"conv4_1\", \"relu4_1\", \"conv4_2\", \"relu4_2\", \"conv4_3\", \"relu4_3\",\"maxpool4\", \"conv5_1\", \"relu5_1\", \"conv5_2\", \"relu5_2\", \"conv5_3\", \"relu5_3\",\"maxpool5\"]\n",
        "\n",
        "# Choose the layers to use for style and content transfer\n",
        "content_layers = ['conv5_3']\n",
        "style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
        "content_indeces = [vgg_names.index(name) for name in content_layers]\n",
        "style_indeces = [vgg_names.index(name) for name in style_layers]\n",
        "\n",
        "# Create the vgg network in eval mode\n",
        "#  with our forward method that returns the outputs of the intermediate layers we requested\n",
        "vgg_style = VGGIntermediate(style_layers)\n",
        "vgg_content = VGGIntermediate(content_layers)\n",
        "\n",
        "vgg_generated = VGGIntermediate(content_layers.extend(style_layers))\n",
        "\n",
        "\n",
        "\n",
        "# Cache the outputs of the content and style layers for their respective images\n",
        "style_outputs = vgg_style(style_image)\n",
        "content_outputs = vgg_content(content_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHCL3p9-TYWv"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 2\n",
        "Create a method to turn a tensor to an image to display\n",
        "\n",
        "**TODO:**\n",
        "* Display the style tensor and content tensor transformed back to an image\n",
        "\n",
        "**DONE:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "msyhi4HofaR9",
        "colab": {}
      },
      "source": [
        "toPIL = transforms.ToPILImage()  \n",
        "\n",
        "def display(tensor, title=None):\n",
        "    image = tensor.cpu().clone()  \n",
        "    image = image.squeeze(0)    \n",
        "    image = toPIL(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "plt.figure()\n",
        "display(style_image, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "display(content_image, title='Content Image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-3GS-qHTp0a"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 3\n",
        "Create a classes for the style and content loss\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Create a module that calculates the content loss in the forward method, compared to some precalculated targets stored in the class\n",
        "* Create a module that calculates the style loss in the forward method using a gram matrix, compared to some precalculated targets stored in the class\n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AjyA3hK7koqs",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "    b, f, h, w = input.size()  \n",
        "    features = input.reshape((b * f, h * w))\n",
        "    gram = torch.mm(features, features.t())\n",
        "    return gram\n",
        "  \n",
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self, target):\n",
        "    super(ContentLoss, self).__init__()\n",
        "    self.target = target.detach()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.mse_loss(x, self.target)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self, target):\n",
        "    self.target = gram_matrix(target).detach()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    gram = gram_matrix(x)\n",
        "    return F.mse_loss(gram, self.target)\n",
        "\n",
        "# Instantiate a content loss module for each content layer \n",
        "#  with the content reference image outputs for that layer for comparison\n",
        "\n",
        "content_loss_modules = {layer: ContentLoss(content_outputs[layer] for layer in content_indeces)\n",
        "# Instantiate a sytle loss module for each style layer \n",
        "#  with the style reference image outputs for that layer for comparison\n",
        "style_loss_modules = {layer: StyleLoss(style_outputs[layer]) for layer in style_indeces}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z68Rq165ULGj"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 4\n",
        "Create and run a method that minimizes the content and style loss for a copy of the content image\n",
        "\n",
        "Note that the content loss should be zero if you take out the style loss. Why is that?\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Use an Adam optimizer with learning rate of .1\n",
        "* Show both the content and the style loss every 50 steps\n",
        "* Ensure that the outputs don't go out of range (clamp them)\n",
        "* Display the tensor as an image!\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zflDDObAflen",
        "colab": {}
      },
      "source": [
        "# Start with a copy of the content image\n",
        "input_image = content_image.clone()\n",
        "\n",
        "# Run the optimizer on the images to change the image\n",
        "#  using the loss of the style and content layers\n",
        "#  to backpropagate errors \n",
        "\n",
        "def style_transfer()\n",
        "  optimizer = optim.Adam([input_img.requires_grad_()], lr=0.1)\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  optimizer.zero_grad()\n",
        "  input_image.data.clamp_(0, 1)\n",
        "  imgs = vgg_generated(input_image)\n",
        "  content_loss = 0\n",
        "  for layer in content_indeces:\n",
        "    content_loss += content_loss_modules[layer](imgs[layer])\n",
        "    \n",
        "  style_loss = 0\n",
        "  for layer in style_indeces:\n",
        "    style_loss += style_loss_modules[layer](imgs[layer])\n",
        "  \n",
        "  loss = content_loss * w_cl + style_loss * w_sl\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "input_image.data.clamp_(0, 1)\n",
        "display(input_image)\n",
        "#test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1qr7O2wpSq8",
        "colab_type": "text"
      },
      "source": [
        "### Part 5\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "- Describe equation (1) on p. 10 of (Gatys 2016) in plain English, including the meaning of all variables and subscripts.\n",
        "- Describe equation (3) on p. 11 of (Gatys 2016) in plain English, including the meaning of all variables and subscripts.\n",
        "- Describe equation (4) on p. 11 of (Gatys 2016) in plain English, including the meaning of all variables and subscripts.\n",
        "- Describe equation (5) on p. 11 of (Gatys 2016) in plain English, including the meaning of all variables and subscripts.\n",
        "\n",
        "**DONE:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJmHNh0yrfVp",
        "colab_type": "text"
      },
      "source": [
        "### Equation (1) on p. 10\n",
        " - p and x are the content image and the image being generated, respectively\n",
        " - l is the current layer in the network\n",
        " - i corresponds to the filter in layer l\n",
        " - j corresponds to the position in filter i\n",
        " - P is the feature representation of p in the current layer, meaning the output of the current layer for the content image\n",
        " - F is the feature representation of x in the current layer, meaning the output of the current layer for the generated image\n",
        " \n",
        " The equation as a whole is the mean squared error loss between the feature representations of the content image and the generated image. This is used to determine the loss in terms of content and improve the content quality of the generated image by acting as a distance between the content image and the generated image in terms of content.\n",
        "\n",
        "\n",
        "### Equation (3) on p. 11\n",
        " - i and j are the indeces of the feature maps that are being compared\n",
        " - k is the current index inside of each feature map\n",
        " - l is the current layer in the network, corresponding to a list of feature maps\n",
        " - F just means its the feature map corresponding to i or j\n",
        " \n",
        " This equation is an inner product, or basically a pointwise multiply followed by a sum over the resulting matrix. (Look up more on gram matrices and see how all the feature maps are combined to make the gram matrix). Gij corresponds to one position in the final Gram matrix G for that layer.\n",
        " \n",
        " \n",
        "### Equation (4) on p. 11\n",
        " - l is the current layer of the network\n",
        " - i and j correspond to a position in the matrices G and A\n",
        " - G is the feature representation of the generated image in the current layer\n",
        " - A is the feature representation of the style image in the current layer\n",
        " - N is the number of distinct filters/feature maps in the current layer\n",
        " - M is the size of each feature map in the current layer, given by the height multiplied by the width of the feature map.\n",
        " \n",
        " This equation represents the amount that layer l contributes to the total style loss. It is measured similarly to the content loss, but instead of measuring distance between the generated image and the original image, it is measured between the generated image and the gram matrix of the original image. It is also divided by four(???) times the total number of elements in the feature maps in layer l.\n",
        " \n",
        "### Equation (5) on p. 11\n",
        " - a and x are the style image and the generated image, respectively\n",
        " - l is the current layer\n",
        " - w is a weighting factor for each layer that shows the contribution of each layer to the total loss (???)\n",
        " - E is the output of the previous equation for layer l\n",
        " \n",
        " This equation is essentially a weighted sum of the contribution of each layer to the overall style loss. This is used to determine the distance from the original style image to the generated image in terms of style, and to help modify the generated image to fit better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYgTz1D_uoWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}